{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad420cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41da514a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/04 17:19:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2411ea93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-02-28 20:09:30--  https://s3.amazonaws.com/nyc-tlc/misc/taxi+_zone_lookup.csv\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.36.104, 52.217.234.128, 52.217.236.160, ...\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.36.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 12322 (12K) [application/octet-stream]\n",
      "Saving to: ‘taxi+_zone_lookup.csv.2’\n",
      "\n",
      "taxi+_zone_lookup.c 100%[===================>]  12.03K  --.-KB/s    in 0s      \n",
      "\n",
      "2023-02-28 20:09:31 (34.4 MB/s) - ‘taxi+_zone_lookup.csv.2’ saved [12322/12322]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!wget https://s3.amazonaws.com/nyc-tlc/misc/taxi+_zone_lookup.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "807d9512",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_taxi = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\",True)\\\n",
    "    .csv('taxi+_zone_lookup.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b776fe4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------------------+------------+\n",
      "|LocationID|      Borough|                Zone|service_zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "|         1|          EWR|      Newark Airport|         EWR|\n",
      "|         2|       Queens|         Jamaica Bay|   Boro Zone|\n",
      "|         3|        Bronx|Allerton/Pelham G...|   Boro Zone|\n",
      "|         4|    Manhattan|       Alphabet City| Yellow Zone|\n",
      "|         5|Staten Island|       Arden Heights|   Boro Zone|\n",
      "|         6|Staten Island|Arrochar/Fort Wad...|   Boro Zone|\n",
      "|         7|       Queens|             Astoria|   Boro Zone|\n",
      "|         8|       Queens|        Astoria Park|   Boro Zone|\n",
      "|         9|       Queens|          Auburndale|   Boro Zone|\n",
      "|        10|       Queens|        Baisley Park|   Boro Zone|\n",
      "|        11|     Brooklyn|          Bath Beach|   Boro Zone|\n",
      "|        12|    Manhattan|        Battery Park| Yellow Zone|\n",
      "|        13|    Manhattan|   Battery Park City| Yellow Zone|\n",
      "|        14|     Brooklyn|           Bay Ridge|   Boro Zone|\n",
      "|        15|       Queens|Bay Terrace/Fort ...|   Boro Zone|\n",
      "|        16|       Queens|             Bayside|   Boro Zone|\n",
      "|        17|     Brooklyn|             Bedford|   Boro Zone|\n",
      "|        18|        Bronx|        Bedford Park|   Boro Zone|\n",
      "|        19|       Queens|           Bellerose|   Boro Zone|\n",
      "|        20|        Bronx|             Belmont|   Boro Zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_taxi.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fb439054",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('LocationID', IntegerType(), True), StructField('Borough', StringType(), True), StructField('Zone', StringType(), True), StructField('service_zone', StringType(), True)])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_taxi.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "581463d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- LocationID: integer (nullable = true)\n",
      " |-- Borough: string (nullable = true)\n",
      " |-- Zone: string (nullable = true)\n",
      " |-- service_zone: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_taxi.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "30823fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .csv(\"fhv_tripdata_2019-12.csv.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "337f5dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropOff_datetime: timestamp (nullable = true)\n",
      " |-- PUlocationID: integer (nullable = true)\n",
      " |-- DOlocationID: integer (nullable = true)\n",
      " |-- SR_Flag: string (nullable = true)\n",
      " |-- Affiliated_base_number: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "434105c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c2e58fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_udf = F.udf(lambda x: str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aafde1f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 16:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+---------+\n",
      "|dispatching_base_num|    pickup_datetime|   dropOff_datetime|PUlocationID|DOlocationID|SR_Flag|Affiliated_base_number|str_pu_id|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+---------+\n",
      "|              B00009|2019-12-01 00:47:00|2019-12-01 00:52:00|         264|         264|   null|                B00009|      264|\n",
      "|              B00009|2019-12-01 00:27:00|2019-12-01 00:44:00|         264|         264|   null|                B00009|      264|\n",
      "|              B00014|2019-12-01 00:42:18|2019-12-01 01:31:31|         264|         264|   null|                B00014|      264|\n",
      "|              B00014|2019-12-01 00:43:08|2019-12-01 01:07:38|         264|         264|   null|                B00014|      264|\n",
      "|     B00021         |2019-12-01 00:52:19|2019-12-01 00:59:39|          56|          56|   null|       B00021         |       56|\n",
      "|     B00021         |2019-12-01 00:49:01|2019-12-01 00:59:43|          82|          82|   null|       B00021         |       82|\n",
      "|     B00021         |2019-12-01 00:11:25|2019-12-01 00:20:38|          93|          95|   null|       B00021         |       93|\n",
      "|     B00021         |2019-12-01 00:32:52|2019-12-01 00:39:15|         173|         173|   null|       B00021         |      173|\n",
      "|     B00021         |2019-12-01 00:10:52|2019-12-01 00:33:08|          83|           7|   null|       B00021         |       83|\n",
      "|     B00021         |2019-12-01 00:34:27|2019-12-01 00:42:22|          56|          56|   null|       B00021         |       56|\n",
      "|              B00037|2019-12-01 00:10:16|2019-12-01 00:21:08|         264|          71|   null|                B00037|      264|\n",
      "|              B00037|2019-12-01 00:26:12|2019-12-01 00:34:43|         264|          91|   null|                B00037|      264|\n",
      "|              B00037|2019-12-01 00:48:10|2019-12-01 00:55:15|         264|          39|   null|                B00037|      264|\n",
      "|              B00037|2019-12-01 00:18:48|2019-12-01 00:26:32|         264|          91|   null|                B00037|      264|\n",
      "|              B00037|2019-12-01 00:50:12|2019-12-01 00:57:49|         264|          72|   null|                B00037|      264|\n",
      "|              B00037|2019-12-01 00:09:12|2019-12-01 00:21:12|         264|          39|   null|                B00037|      264|\n",
      "|              B00037|2019-12-01 00:20:39|2019-12-01 00:25:54|         264|         188|   null|                B00037|      264|\n",
      "|              B00037|2019-12-01 00:33:59|2019-12-01 00:38:32|         264|          61|   null|                B00037|      264|\n",
      "|              B00037|2019-12-01 00:53:13|2019-12-01 00:56:47|         264|         188|   null|                B00037|      264|\n",
      "|              B00037|2019-12-01 00:28:10|2019-12-01 00:30:27|         264|          71|   null|                B00037|      264|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"str_pu_id\",string_udf(df.PUlocationID)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "83123165",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a498aeb3",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "path file:/Users/soapycat/Documents/Persona/DataZooCamp/week_5_spark/test_fhv_12 already exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepartition\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest_fhv_12\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.asdf/installs/python/3.10.9/lib/python3.10/site-packages/pyspark/sql/readwriter.py:1140\u001b[0m, in \u001b[0;36mDataFrameWriter.parquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartitionBy(partitionBy)\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(compression\u001b[38;5;241m=\u001b[39mcompression)\n\u001b[0;32m-> 1140\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.asdf/installs/python/3.10.9/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/.asdf/installs/python/3.10.9/lib/python3.10/site-packages/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: path file:/Users/soapycat/Documents/Persona/DataZooCamp/week_5_spark/test_fhv_12 already exists."
     ]
    }
   ],
   "source": [
    "#df.repartition(4).write.parquet(\"test_fhv_12\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ef7b7afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_12 = spark.read.parquet('test_fhv_12/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b7d6d49f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_6_before = spark.read.option(\"header\",True)\\\n",
    "    .option(\"inferSchema\",True)\\\n",
    "    .csv('fhv_tripdata_2019-06.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1073e154",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_6_before.repartition(4).write.parquet(\"test_fhv_6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f505c054",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_6 = spark.read.parquet('test_fhv_6/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f3e5751f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dispatching_base_num',\n",
       " 'pickup_datetime',\n",
       " 'dropOff_datetime',\n",
       " 'PUlocationID',\n",
       " 'DOlocationID',\n",
       " 'SR_Flag',\n",
       " 'Affiliated_base_number']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_6.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b9e9fbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_cols = list(set(df_6.columns) & set(df_12.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6c0f81e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_6_sel = df_6.select(common_cols).withColumn(\"month\",F.lit(\"June\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "490e5b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_12_sel = df_12.select(common_cols).withColumn(\"month\",F.lit(\"December\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0c7702cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fhv_all = df_6_sel.unionAll(df_12_sel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f95de985",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 21:==============>                                          (4 + 8) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+\n",
      "|   month|  count|\n",
      "+--------+-------+\n",
      "|    June|2009886|\n",
      "|December|2044196|\n",
      "+--------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_fhv_all.groupBy('month').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2cc056cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fhv_all.createOrReplaceTempView(\"fhv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "38a1f811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|   month|count(1)|\n",
      "+--------+--------+\n",
      "|    June| 2009886|\n",
      "|December| 2044196|\n",
      "+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql_result = spark.sql(\"\"\"\n",
    "    select month, count(1) from fhv group by 1\n",
    "\"\"\")\n",
    "\n",
    "sql_result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7f8fe1bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|   month|count(1)|\n",
      "+--------+--------+\n",
      "|    June| 2009886|\n",
      "|December| 2044196|\n",
      "+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql_result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "520ba5b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_result.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ec623dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------------+-------------------+-------------------+-------+------------+------------+-----+\n",
      "|dispatching_base_num|Affiliated_base_number|    pickup_datetime|   dropOff_datetime|SR_Flag|PUlocationID|DOlocationID|month|\n",
      "+--------------------+----------------------+-------------------+-------------------+-------+------------+------------+-----+\n",
      "|              B01437|                B02872|2019-06-20 22:29:42|2019-06-20 22:35:06|   null|         264|         265| June|\n",
      "|              B01087|                B01087|2019-06-17 19:03:43|2019-06-17 20:20:43|   null|          65|          55| June|\n",
      "|              B00900|                B00900|2019-06-05 16:19:02|2019-06-05 16:22:20|   null|         264|         265| June|\n",
      "|              B01469|                B02867|2019-06-26 09:11:35|2019-06-26 09:22:23|   null|         264|         265| June|\n",
      "|              B01432|                B01432|2019-06-23 19:02:35|2019-06-23 19:14:38|   null|         264|         264| June|\n",
      "+--------------------+----------------------+-------------------+-------------------+-------+------------+------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_fhv_all.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "aa62b01a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------------------+------------+\n",
      "|LocationID|      Borough|                Zone|service_zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "|         1|          EWR|      Newark Airport|         EWR|\n",
      "|         2|       Queens|         Jamaica Bay|   Boro Zone|\n",
      "|         3|        Bronx|Allerton/Pelham G...|   Boro Zone|\n",
      "|         4|    Manhattan|       Alphabet City| Yellow Zone|\n",
      "|         5|Staten Island|       Arden Heights|   Boro Zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_taxi.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7c0c86b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------------+-------------------+-------------------+-------+------------+------------+-----+----------+--------+--------------------+------------+\n",
      "|dispatching_base_num|Affiliated_base_number|    pickup_datetime|   dropOff_datetime|SR_Flag|PUlocationID|DOlocationID|month|LocationID| Borough|                Zone|service_zone|\n",
      "+--------------------+----------------------+-------------------+-------------------+-------+------------+------------+-----+----------+--------+--------------------+------------+\n",
      "|              B01437|                B02872|2019-06-20 22:29:42|2019-06-20 22:35:06|   null|         264|         265| June|       264| Unknown|                  NV|         N/A|\n",
      "|              B01087|                B01087|2019-06-17 19:03:43|2019-06-17 20:20:43|   null|          65|          55| June|        65|Brooklyn|Downtown Brooklyn...|   Boro Zone|\n",
      "|              B00900|                B00900|2019-06-05 16:19:02|2019-06-05 16:22:20|   null|         264|         265| June|       264| Unknown|                  NV|         N/A|\n",
      "|              B01469|                B02867|2019-06-26 09:11:35|2019-06-26 09:22:23|   null|         264|         265| June|       264| Unknown|                  NV|         N/A|\n",
      "|              B01432|                B01432|2019-06-23 19:02:35|2019-06-23 19:14:38|   null|         264|         264| June|       264| Unknown|                  NV|         N/A|\n",
      "+--------------------+----------------------+-------------------+-------------------+-------+------------+------------+-----+----------+--------+--------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_fhv_all.join(df_taxi\n",
    "                , on = df_fhv_all.PUlocationID == df_taxi.LocationID\n",
    "                , how='left').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ab82a96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_green = spark.read\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .parquet(\"green_tripdata_2019-01.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "474c45a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_green = df_green\\\n",
    "    .select('lpep_pickup_datetime', 'PULocationID', 'total_amount')\\\n",
    "    .rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8330197c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(lpep_pickup_datetime=datetime.datetime(2018, 12, 21, 22, 17, 29), PULocationID=264, total_amount=4.3),\n",
       " Row(lpep_pickup_datetime=datetime.datetime(2019, 1, 1, 7, 10, 16), PULocationID=97, total_amount=7.3),\n",
       " Row(lpep_pickup_datetime=datetime.datetime(2019, 1, 1, 7, 27, 11), PULocationID=49, total_amount=5.8),\n",
       " Row(lpep_pickup_datetime=datetime.datetime(2019, 1, 1, 7, 46, 20), PULocationID=189, total_amount=19.71),\n",
       " Row(lpep_pickup_datetime=datetime.datetime(2019, 1, 1, 7, 19, 6), PULocationID=82, total_amount=19.3),\n",
       " Row(lpep_pickup_datetime=datetime.datetime(2019, 1, 1, 7, 12, 35), PULocationID=49, total_amount=7.8),\n",
       " Row(lpep_pickup_datetime=datetime.datetime(2019, 1, 1, 7, 47, 55), PULocationID=255, total_amount=14.8),\n",
       " Row(lpep_pickup_datetime=datetime.datetime(2019, 1, 1, 7, 12, 47), PULocationID=76, total_amount=17.3),\n",
       " Row(lpep_pickup_datetime=datetime.datetime(2019, 1, 1, 7, 16, 23), PULocationID=25, total_amount=26.8),\n",
       " Row(lpep_pickup_datetime=datetime.datetime(2019, 1, 1, 7, 58, 2), PULocationID=85, total_amount=16.8)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_green.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0c01c2eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "630918"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_green.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c1ba674f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_grouping(row): \n",
    "    hour = row.lpep_pickup_datetime.replace(minute=0, second=0, microsecond=0)\n",
    "    zone = row.PULocationID\n",
    "    key = (hour, zone)\n",
    "    \n",
    "    amount = row.total_amount\n",
    "    count = 1\n",
    "    value = (amount, count)\n",
    "\n",
    "    return (key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4f9490b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_revenue(left_value, right_value):\n",
    "    left_amount, left_count = left_value\n",
    "    right_amount, right_count = right_value\n",
    "    \n",
    "    output_amount = left_amount + right_amount\n",
    "    output_count = left_count + right_count\n",
    "    \n",
    "    return (output_amount, output_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "fddd9ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "RevenueRow = namedtuple('RevenueRow', ['hour', 'zone', 'revenue', 'count'])\n",
    "def unwrap(row):\n",
    "    return RevenueRow(\n",
    "        hour=row[0][0], \n",
    "        zone=row[0][1],\n",
    "        revenue=row[1][0],\n",
    "        count=row[1][1]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c10dffd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_result = rdd_green\\\n",
    "    .filter(lambda row: row.total_amount > 10)\\\n",
    "    .map(prepare_for_grouping) \\\n",
    "    .reduceByKey(calculate_revenue) \\\n",
    "    .map(unwrap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9851137d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[RevenueRow(hour=datetime.datetime(2019, 1, 1, 7, 0), zone=76, revenue=256.92, count=8),\n",
       " RevenueRow(hour=datetime.datetime(2019, 1, 1, 7, 0), zone=85, revenue=131.46, count=4),\n",
       " RevenueRow(hour=datetime.datetime(2019, 1, 1, 7, 0), zone=146, revenue=49.91, count=3),\n",
       " RevenueRow(hour=datetime.datetime(2019, 1, 1, 7, 0), zone=181, revenue=1709.5899999999992, count=57),\n",
       " RevenueRow(hour=datetime.datetime(2019, 1, 1, 7, 0), zone=41, revenue=988.6899999999993, count=55)]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_result.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b64ae97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = rdd_result.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "49e56b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+------------------+-----+\n",
      "|               hour|zone|           revenue|count|\n",
      "+-------------------+----+------------------+-----+\n",
      "|2019-01-01 07:00:00|  76|            256.92|    8|\n",
      "|2019-01-01 07:00:00|  85|            131.46|    4|\n",
      "|2019-01-01 07:00:00| 146|             49.91|    3|\n",
      "|2019-01-01 07:00:00| 181|1709.5899999999992|   57|\n",
      "|2019-01-01 07:00:00|  41| 988.6899999999993|   55|\n",
      "+-------------------+----+------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_result.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4862cf29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
